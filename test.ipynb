{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_projection.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'logit_scale', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.final_layer_norm.weight', 'visual_projection.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from llava.conversation import Conversation, SeparatorStyle, conv_templates\n",
    "from llava.utils import disable_torch_init\n",
    "from transformers import CLIPImageProcessor, StoppingCriteria\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import http.server\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
    "\n",
    "\n",
    "CONVERSATION_PROMPT = Conversation(\n",
    "    system=\"You are an assistant that is able to understand the visual content that the user provides. \"\n",
    "    \"You answer questions about the visual content in a short and concise manner.\",\n",
    "    roles=(\"Human\", \"Assistant\"),\n",
    "    messages=(),\n",
    "    offset=2,\n",
    "    sep_style=SeparatorStyle.SINGLE,\n",
    "    sep=\"###\",\n",
    ")\n",
    "\n",
    "\n",
    "params_path = \"../llava-weights/\"\n",
    "# load model\n",
    "disable_torch_init()\n",
    "tokenizer = AutoTokenizer.from_pretrained(params_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    params_path, torch_dtype=torch.float16\n",
    ").cuda()\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\n",
    "    model.config.mm_vision_tower, torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n",
    "tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n",
    "if mm_use_im_start_end:\n",
    "    tokenizer.add_tokens(\n",
    "        [DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True\n",
    "    )\n",
    "\n",
    "vision_tower = model.model.vision_tower[0]\n",
    "vision_tower.to(device=\"cuda\", dtype=torch.float16)\n",
    "vision_config = vision_tower.config\n",
    "vision_config.im_patch_token = tokenizer.convert_tokens_to_ids(\n",
    "    [DEFAULT_IMAGE_PATCH_TOKEN]\n",
    ")[0]\n",
    "vision_config.use_im_start_end = mm_use_im_start_end\n",
    "if mm_use_im_start_end:\n",
    "    (\n",
    "        vision_config.im_start_token,\n",
    "        vision_config.im_end_token,\n",
    "    ) = tokenizer.convert_tokens_to_ids(\n",
    "        [DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN]\n",
    "    )\n",
    "image_token_len = (vision_config.image_size // vision_config.patch_size) ** 2\n",
    "\n",
    "if mm_use_im_start_end:\n",
    "    image_tokens = (\n",
    "        DEFAULT_IM_START_TOKEN\n",
    "        + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\n",
    "        + DEFAULT_IM_END_TOKEN\n",
    "    )\n",
    "else:\n",
    "    image_tokens = DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,     2,     2,     2,     2,     2,     2,     1,  1724,  2277,\n",
      "         29937],\n",
      "        [    1, 11644,   338,   471,  2790,   278,   270, 17006, 29973,  2277,\n",
      "         29937],\n",
      "        [    2,     1,  1724,   338,   278,  1601,  1989,  2599, 29973,  2277,\n",
      "         29937],\n",
      "        [    1,  1724,  2927, 29915, 29879,   278,  1601,  1989, 29973,  2277,\n",
      "         29937]], device='cuda:0')\n",
      "[['The image shows a monkey inside a kitchen, washing dishes in a sink. The monkey is surrounded by various kitchen items, including multiple cups and bowls. Some of the cups are placed near the sink,', 'A monkey is washing the dishes in the image.'], ['The monkey is washing dishes in a kitchen sink, specifically washing a blue bowl.', 'The monkey is brown.']]\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "PROMPT = \"You are an assistant that is able to understand the visual content that the user provides. \"\n",
    "\"You answer questions about the visual content in a short and concise manner.\\n###Human: \"\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    images = [Image.open(\"monkey.jpg\"), Image.open(\"monkey.jpg\")]\n",
    "    queries = [\n",
    "        [\"What\", \"Who is washing the dishes?\"],\n",
    "        [\"What is the monkey doing?\", \"What color's the monkey?\"],\n",
    "    ]\n",
    "\n",
    "    assert len(images) == len(queries)\n",
    "    assert np.all([len(queries[0]) == len(q) for q in queries])\n",
    "\n",
    "    queries = np.array(queries)  # (batch_size, num_queries_per_image)\n",
    "\n",
    "    # preprocess images\n",
    "    images = image_processor(images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    images = images.to(\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    # first, get the activations for the image tokens\n",
    "    initial_prompts = [PROMPT + image_tokens + \"\\n\" for _ in range(len(images))]\n",
    "    initial_input_ids = tokenizer(initial_prompts, return_tensors=\"pt\").input_ids.cuda()\n",
    "    initial_out = model(initial_input_ids, images=images, use_cache=True)\n",
    "    initial_key_values = initial_out.past_key_values\n",
    "    attention_mask = (initial_input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "    # broadcast the key values across the queries\n",
    "    # becomes shape (batch_size * num_queries_per_image, ...)\n",
    "    initial_key_values = [\n",
    "        [\n",
    "            x.unsqueeze(1)\n",
    "            .expand(-1, queries.shape[1], -1, -1, -1)\n",
    "            .reshape(-1, *x.shape[1:])\n",
    "            for x in y\n",
    "        ]\n",
    "        for y in initial_key_values\n",
    "    ]\n",
    "\n",
    "    # broadcast the attention mask across the queries\n",
    "    # becomes shape (batch_size * num_queries_per_image, ...)\n",
    "    attention_mask = (\n",
    "        attention_mask.unsqueeze(1)\n",
    "        .expand(-1, queries.shape[1], -1)\n",
    "        .reshape(-1, attention_mask.shape[1])\n",
    "    )\n",
    "\n",
    "    # flatten queries into one big batch\n",
    "    queries_flat = queries.reshape(-1)  # (batch_size * num_queries_per_image)\n",
    "\n",
    "    # prepare inputs for the queries\n",
    "    prompts = [q + \"###\" for q in queries_flat]\n",
    "    input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True).input_ids.cuda()\n",
    "    print(input_ids)\n",
    "\n",
    "    # stop upon seeing any of these tokens\n",
    "    stop_tokens = torch.as_tensor(\n",
    "        tokenizer.convert_tokens_to_ids([\"▁###\", \"##\", \"#\"]),\n",
    "        dtype=torch.long,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "\n",
    "    # generation loop\n",
    "    output_ids = []\n",
    "    key_values = initial_key_values\n",
    "    finished = torch.zeros(input_ids.shape[0], dtype=torch.bool, device=\"cuda\")\n",
    "    for i in range(50):\n",
    "        attention_mask = torch.cat(\n",
    "            [attention_mask, input_ids != tokenizer.pad_token_id], dim=-1\n",
    "        )\n",
    "\n",
    "        # create position_ids on the fly for batch generation\n",
    "        # position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        # position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "        # position_ids = position_ids[:, -input_ids.shape[1]]\n",
    "        # print(position_ids)\n",
    "\n",
    "        # print(attention_mask[0, -10:])\n",
    "        out = model(\n",
    "            input_ids=input_ids,\n",
    "            use_cache=True,\n",
    "            past_key_values=key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            # position_ids=position_ids,\n",
    "        )\n",
    "        key_values = out.past_key_values\n",
    "        next_tokens = torch.argmax(out.logits[:, -1], dim=-1)\n",
    "\n",
    "        next_tokens = torch.where(finished, tokenizer.pad_token_id, next_tokens)\n",
    "\n",
    "        finished = finished | (next_tokens.unsqueeze(-1) == stop_tokens).any(dim=-1)\n",
    "\n",
    "        if finished.all():\n",
    "            break\n",
    "\n",
    "        output_ids.append(next_tokens)\n",
    "        input_ids = next_tokens.unsqueeze(-1)\n",
    "\n",
    "output_ids = torch.stack(output_ids, dim=-1)\n",
    "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "# clean outputs\n",
    "outputs_clean = []\n",
    "for output in outputs:\n",
    "    for pattern in [\"###\", \"##\", \"#\"]:\n",
    "        if pattern in output:\n",
    "            output = output.split(pattern)[0]\n",
    "\n",
    "    if \"Assistant:\" in output:\n",
    "        output = output.split(\"Assistant:\")[1]\n",
    "    outputs_clean.append(output.strip())\n",
    "\n",
    "# reshape outputs back to (batch_size, num_queries_per_image)\n",
    "outputs_clean = np.array(outputs_clean).reshape(queries.shape)\n",
    "\n",
    "print(outputs_clean.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,  1724,   338,   278,  1601,  1989,  2599, 29973,  2277, 29937,\n",
      "            2])\n",
      "tensor([    1,  1724,  2927, 29915, 29879,   278,  1601,  1989, 29973,  2277,\n",
      "        29937])\n",
      " What is the monkey doing?###\n",
      " What color's the monkey?###\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True).input_ids.cpu()\n",
    "print(input_ids[0])\n",
    "print(input_ids[1])\n",
    "print(tokenizer.decode(input_ids[0, :-1], skip_special_tokens=False))\n",
    "print(tokenizer.decode(input_ids[1], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopping_criteria' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     19\u001b[0m input_ids \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, queries\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m     22\u001b[0m     output_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m     23\u001b[0m         input_ids,\n\u001b[1;32m     24\u001b[0m         images\u001b[39m=\u001b[39mimage_tensors,\n\u001b[1;32m     25\u001b[0m         do_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m         temperature\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m,\n\u001b[1;32m     27\u001b[0m         max_new_tokens\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m,\n\u001b[0;32m---> 28\u001b[0m         stopping_criteria\u001b[39m=\u001b[39m[stopping_criteria(tokenizer, input_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])],\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     31\u001b[0m output_ids \u001b[39m=\u001b[39m output_ids[:, input_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] :]\n\u001b[1;32m     32\u001b[0m \u001b[39m# input_token_len = input_ids.shape[1]\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# n_diff_input_output = (\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m#     (input_ids != output_ids[:, :input_token_len]).sum().item()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m#         f\"[Warning] {n_diff_input_output} output_ids are not the same as the input_ids\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m#     )\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopping_criteria' is not defined"
     ]
    }
   ],
   "source": [
    "images = [Image.open(\"monkey.jpg\")]\n",
    "queries = [[\"What is the color of the monkey?\", \"What is the monkey doing?\"]]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    queries = np.array(queries)\n",
    "    prompts = []\n",
    "    for query_set in queries:\n",
    "        message = image_tokens + \"\\n\" + query_set[0]\n",
    "        conv = CONVERSATION_PROMPT.copy()\n",
    "        conv.append_message(conv.roles[0], message)\n",
    "        prompts.append(conv.get_prompt())\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "    image_tensors = (\n",
    "        image_processor.preprocess(images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        .half()\n",
    "        .cuda()\n",
    "    )\n",
    "    input_ids = inputs.input_ids.cuda()\n",
    "\n",
    "    for i in range(1, queries.shape[1]):\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=image_tensors,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            max_new_tokens=128,\n",
    "            stopping_criteria=[stopping_criteria(tokenizer, input_ids.shape[0])],\n",
    "        )\n",
    "\n",
    "    output_ids = output_ids[:, input_ids.shape[1] :]\n",
    "    # input_token_len = input_ids.shape[1]\n",
    "    # n_diff_input_output = (\n",
    "    #     (input_ids != output_ids[:, :input_token_len]).sum().item()\n",
    "    # )\n",
    "    # if n_diff_input_output > 0:\n",
    "    #     print(\n",
    "    #         f\"[Warning] {n_diff_input_output} output_ids are not the same as the input_ids\"\n",
    "    #     )\n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "    outputs_clean = []\n",
    "    for output in outputs:\n",
    "        for pattern in [\"###\", \"##\", \"#\"]:\n",
    "            if pattern in output:\n",
    "                output = output.split(pattern)[0]\n",
    "        if \"Assistant:\" in output:\n",
    "            output = output.split(\"Assistant:\")[1]\n",
    "        outputs_clean.append(output.strip())\n",
    "\n",
    "    # try:\n",
    "    #     index = outputs.index(conv.sep)\n",
    "    # except ValueError:\n",
    "    #     outputs += conv.sep\n",
    "    #     index = outputs.index(conv.sep)\n",
    "\n",
    "    print(outputs_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
